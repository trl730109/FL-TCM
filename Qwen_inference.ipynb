{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tangzichen/miniconda3/envs/fedllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-06 09:11:17,764] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/home/tangzichen/models/Qwen1.5-0.5B-Chat\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/tangzichen/models/Qwen1.5-0.5B-Chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载Lora模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm()\n",
       "            (post_attention_layernorm): Qwen2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_model = PeftModel.from_pretrained(model, model_id=\"/home/tangzichen/OpenFedLLM/output/ShenNong_TCM_Dataset_0_fedavg_c8s4_i-1_b8a1_l512_r32a64_20241105201826/checkpoint-5000\")\n",
    "p_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 舌头边缘锯齿状怎么回事，请从中医角度回答\\n\\nAssistant: 从中医的角度来看，舌唇边缘的锯齿状可能是由于口腔溃疡、口腔炎、口干等引起的。在中医中，常见的病因包括肝火上扰、胃热伤阴、气血不足等。建议您到医院或中医诊所进行详细的诊断和治疗。同时，注意口腔卫生，避免辛辣刺激性食物和饮料，保持口腔湿润，多喝水，适当休息，有助于缓解症状。<|im_end|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"舌头边缘锯齿状怎么回事，请从中医角度回答\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\")\n",
    "# tokenizer.decode(p_model.generate(**ipt, do_sample=False)[0], skip_special_tokens=True)\n",
    "tokenizer.decode(p_model.generate(**ipt, max_length=512)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 舌头边缘锯齿状怎么回事，请从中医角度回答\\n\\nAssistant: 从中医的角度来看，舌头上出现锯齿状的现象，通常是因为体内湿气较重、气血不畅或者脾胃虚弱等因素所致。建议您可以多食用一些清淡易消化的食物，如粥类、蔬菜水果等；同时保持良好的生活习惯，避免过度劳累和情绪波动。如果情况较为严重，建议您咨询专业的中医师进行诊断和治疗。<|im_end|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"舌头边缘锯齿状怎么回事，请从中医角度回答\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\")\n",
    "# tokenizer.decode(p_model.generate(**ipt, do_sample=False)[0], skip_special_tokens=True)\n",
    "tokenizer.decode(model.generate(**ipt, max_length=512)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_model = p_model.merge_and_unload()\n",
    "merge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 考试有哪些技巧？\\n\\nAssistant: 1. 熟悉考试大纲'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\")\n",
    "tokenizer.decode(merge_model.generate(**ipt, do_sample=False)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_model.save_pretrained(\"./chatbot/merge_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
